{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea0a796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Dataset: https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_input = tokenizer(df['headline'].tolist(), return_tensors='pt',padding=True)\n",
    "\n",
    "X = encoded_input['input_ids']\n",
    "y = torch.tensor(df['is_sarcastic'].values).float()\n",
    "\n",
    "#Keeping only the first 10k samples to cut down training time\n",
    "#X=X[:10000,:]\n",
    "#y=y[:10000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62076c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (word_embedding): Embedding(30109, 128)\n",
       "  (position_embedding): Embedding(193, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (classifier2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim, num_heads, dropout, expansion_ratio):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion_ratio*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion_ratio*embed_dim,embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, query):\n",
    "        attention, _ = self.attention(value, key, query)\n",
    "        x=self.dropout(self.norm1(attention+query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out=self.dropout(self.norm2(forward+x))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    #the vocab size is one more than the max value in the X matrix.\n",
    "    def __init__(self,vocab_size=30109,embed_dim=128,num_layers=1,num_heads=4,device=\"cpu\",expansion_ratio=4,dropout=0.1,max_length=193):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length,embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_dim,num_heads,dropout,expansion_ratio) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier1 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.classifier2 = nn.Linear(embed_dim,1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0,seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            #print(out.shape)\n",
    "            out = layer(out,out,out)\n",
    "        \n",
    "        #Get the first output for classification\n",
    "        #Pooled output from hugging face is: Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function.\n",
    "        #Pooled output from hugging face will be different from out[:,0,:], which is the output from the CLS token.\n",
    "        out = self.relu(self.classifier1(out[:,0,:]))\n",
    "        out = self.classifier2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "net = Encoder(device=device)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee23ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only required if debugging for CUDA errors to get an accurate traceback\n",
    "#import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = str(1)\n",
    "\n",
    "batch_size = 32\n",
    "num_train_samples = X_train.shape[0]\n",
    "num_val_samples = X_test.shape[0]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f26c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables that need to be reset for the first training\n",
    "val_loss_hist=[]\n",
    "loss_hist=[]\n",
    "epoch = 0\n",
    "min_val_loss = math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc8c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "Epoch: 1 Train Loss: 124.1192. Val Loss: 123.8753 LR: 1e-05\n",
      "\u001b[93mModel Saved\u001b[0m\n",
      "Epoch: 2 Train Loss: 124.0192. Val Loss: 123.8767 LR: 1e-05\n",
      "Epoch: 3 Train Loss: 124.0183. Val Loss: 123.8744 LR: 1e-05\n",
      "\u001b[93mModel Saved\u001b[0m\n",
      "Epoch: 4 Train Loss: 123.9228. Val Loss: 123.9224 LR: 1e-05\n",
      "Epoch: 5 Train Loss: 123.9748. Val Loss: 123.8779 LR: 1e-05\n",
      "Epoch: 6 Train Loss: 124.0237. Val Loss: 123.8771 LR: 1e-05\n",
      "Epoch: 7 Train Loss: 124.0117. Val Loss: 123.8727 LR: 1.0000000000000002e-06\n",
      "\u001b[93mModel Saved\u001b[0m\n",
      "Epoch: 8 Train Loss: 123.9211. Val Loss: 123.8745 LR: 1.0000000000000002e-06\n",
      "Epoch: 9 Train Loss: 123.9601. Val Loss: 123.8778 LR: 1.0000000000000002e-06\n",
      "Epoch: 10 Train Loss: 123.9187. Val Loss: 123.8783 LR: 1.0000000000000002e-06\n",
      "Epoch: 11 Train Loss: 123.8877. Val Loss: 123.8774 LR: 1.0000000000000002e-06\n",
      "Epoch: 12 Train Loss: 124.0129. Val Loss: 123.8770 LR: 1.0000000000000002e-06\n",
      "Epoch: 13 Train Loss: 124.0220. Val Loss: 123.8778 LR: 1.0000000000000002e-07\n",
      "Epoch: 14 Train Loss: 123.9135. Val Loss: 123.8780 LR: 1.0000000000000002e-07\n",
      "Epoch: 15 Train Loss: 123.9596. Val Loss: 123.8798 LR: 1.0000000000000002e-07\n",
      "Epoch: 16 Train Loss: 123.8878. Val Loss: 123.8798 LR: 1.0000000000000002e-07\n",
      "Epoch: 17 Train Loss: 124.0264. Val Loss: 123.8788 LR: 1.0000000000000002e-07\n",
      "Training Ended\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Started\")\n",
    "\n",
    "patience = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    epoch += 1\n",
    "        \n",
    "    net.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "    \n",
    "    for i in range(0,X_train.size()[0], batch_size):\n",
    "        \n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \n",
    "        features=X_train[indices].to(device)\n",
    "        labels=y_train[indices].reshape(-1,1).to(device)\n",
    "        \n",
    "        output = net.forward(features)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss+=loss.item()\n",
    "        \n",
    "    epoch_loss = epoch_loss / num_train_samples * num_val_samples\n",
    "    loss_hist.append(epoch_loss)\n",
    "    \n",
    "    #print(\"Eval\")\n",
    "    net.eval()\n",
    "    epoch_val_loss = 0\n",
    "    \n",
    "    permutation = torch.randperm(X_test.size()[0])\n",
    "    \n",
    "    for i in range(0,X_test.size()[0], batch_size):\n",
    "        \n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \n",
    "        features=X_test[indices].to(device)\n",
    "        labels = y_test[indices].reshape(-1,1).to(device)\n",
    "        \n",
    "        output = net.forward(features)\n",
    "        loss = criterion(output, labels)        \n",
    "\n",
    "        epoch_val_loss+=loss.item()\n",
    "    \n",
    "    val_loss_hist.append(epoch_val_loss)\n",
    "    \n",
    "    scheduler.step(epoch_val_loss)\n",
    "    \n",
    "    #if epoch % 5 == 0:\n",
    "    print(\"Epoch: \" + str(epoch) + \" Train Loss: \" + format(epoch_loss, \".4f\") + \". Val Loss: \" + format(epoch_val_loss, \".4f\") + \" LR: \" + str(optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "    if epoch_val_loss < min_val_loss:\n",
    "        min_val_loss = epoch_val_loss\n",
    "        torch.save(net.state_dict(), \"torchmodel/weights_best.pth\")\n",
    "        print('\\033[93m'+\"Model Saved\"+'\\033[0m')\n",
    "        patience = 0\n",
    "        \n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if (patience == 10):\n",
    "        break\n",
    "        \n",
    "print(\"Training Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5daa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761],\n",
      "        [0.4761]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.484375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Inference\n",
    "torch.cuda.empty_cache()\n",
    "net = Encoder(device=device)\n",
    "net.load_state_dict(torch.load(\"torchmodel/weights_best.pth\"))\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "#checking on the train set to see if it gives all the same output\n",
    "X=X_test[:64,:]\n",
    "y=y_test[:64]\n",
    "\n",
    "features=X.to(device)\n",
    "\n",
    "output = net.forward(features).detach()\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "output = sigmoid(output)\n",
    "\n",
    "print(output)\n",
    "\n",
    "output = (output>0.5).float().to(\"cpu\")\n",
    "labels = y.reshape(-1,1)\n",
    "\n",
    "accuracy_score(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5d1f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>1</td>\n",
       "      <td>jews to celebrate rosh hashasha or something</td>\n",
       "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>1</td>\n",
       "      <td>internal affairs investigator disappointed con...</td>\n",
       "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>0</td>\n",
       "      <td>the most beautiful acceptance speech this week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>1</td>\n",
       "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
       "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28618</th>\n",
       "      <td>1</td>\n",
       "      <td>dad clarifies this not a food stop</td>\n",
       "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28619 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  thirtysomething scientists unveil doomsday clo...   \n",
       "1                 0  dem rep. totally nails why congress is falling...   \n",
       "2                 0  eat your veggies: 9 deliciously different recipes   \n",
       "3                 1  inclement weather prevents liar from getting t...   \n",
       "4                 1  mother comes pretty close to using word 'strea...   \n",
       "...             ...                                                ...   \n",
       "28614             1       jews to celebrate rosh hashasha or something   \n",
       "28615             1  internal affairs investigator disappointed con...   \n",
       "28616             0  the most beautiful acceptance speech this week...   \n",
       "28617             1  mars probe destroyed by orbiting spielberg-gat...   \n",
       "28618             1                 dad clarifies this not a food stop   \n",
       "\n",
       "                                            article_link  \n",
       "0      https://www.theonion.com/thirtysomething-scien...  \n",
       "1      https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2      https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3      https://local.theonion.com/inclement-weather-p...  \n",
       "4      https://www.theonion.com/mother-comes-pretty-c...  \n",
       "...                                                  ...  \n",
       "28614  https://www.theonion.com/jews-to-celebrate-ros...  \n",
       "28615  https://local.theonion.com/internal-affairs-in...  \n",
       "28616  https://www.huffingtonpost.com/entry/andrew-ah...  \n",
       "28617  https://www.theonion.com/mars-probe-destroyed-...  \n",
       "28618  https://www.theonion.com/dad-clarifies-this-no...  \n",
       "\n",
       "[28619 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing Scripts\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a811eb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length in words are : 151\n"
     ]
    }
   ],
   "source": [
    "res = df['headline'].str.split().str.len().max()\n",
    "print(\"The maximum length in words are : \" +  str(res)) \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_input = tokenizer(df['headline'].tolist(), return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d8a84b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "torch.Size([28619, 193])\n",
      "tensor([  101,  4228, 14045, 20744,  6529,  4895,  3726,  4014, 12677, 16150,\n",
      "         4710,  5119,  1997,  2606,  3279,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0])\n",
      "tensor(30108)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input.keys())\n",
    "print(encoded_input['input_ids'].shape)\n",
    "print(encoded_input['input_ids'][0])\n",
    "print(torch.max(encoded_input['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de78e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor([  101, 11734,  3237, 20057,  2039,  5896,  2008,  4269,  2007,  2996,\n",
      "         8154, 14059,  2046,  7411,  2915,  1997,  5025,  3137,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0])\n",
      "tensor([  101,  2339, 10756,  2024,  2061,  2569,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0])\n",
      "tensor([0., 0., 1.,  ..., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.any(torch.isnan(X_train)))\n",
    "print(torch.any(torch.isnan(X_test)))\n",
    "print(torch.any(torch.isnan(y_train)))\n",
    "print(torch.any(torch.isnan(y_test)))\n",
    "\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "#print(len(y_train))\n",
    "#print(len(y_test))\n",
    "#print(torch.sum(y_train))\n",
    "#print(torch.sum(y_test))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
