## Decision Trees

I try to understand the underlyind algorithms behind Gradient Boosted Decision Trees, specifically XGBoost, since it has been one of the most popular decision tree libraries in research and Kaggle competitions. 

I found that one of the best resources to understand GDBTs are the YouTube videos produced by Josh Starmer. He is probably better known as the StatQuest guy, also the name of his YouTube channel.

The first set of Stat Quests introduce Gradient Boosted Decision Trees: 

1) [Gradient Boost Part 1 (of 4): Regression Main Ideas](https://www.youtube.com/watch?v=3CC4N4z3GJc)

2) [Gradient Boost Part 2 (of 4): Regression Details](https://www.youtube.com/watch?v=2xudPOBz-vs)

3) [Gradient Boost Part 3 (of 4): Classification](https://www.youtube.com/watch?v=jxuNLH5dXCs)

4) [Gradient Boost Part 4 (of 4): Classification Details](https://www.youtube.com/watch?v=StWY5QWMXCw)

The second set introduce XGBoost specifically:

1) [XGBoost Part 1 (of 4): Regression](https://www.youtube.com/watch?v=OtD8wVaFm6E)

2) [XGBoost Part 2 (of 4): Classification](https://www.youtube.com/watch?v=8b1JEDvenQU)

3) [XGBoost Part 3 (of 4): Mathematical Details](https://www.youtube.com/watch?v=ZVFeW798-2I)

4) [XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://www.youtube.com/watch?v=oRrKeUCEbq8)

Following are the research papers on Gradient Boosted Decision Trees:

1) [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)