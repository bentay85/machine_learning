{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c41f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Ideas taken from:\n",
    "# https://github.com/mswang12/minDQN/blob/main/minDQN.py\n",
    "# https://towardsdatascience.com/infinite-steps-cartpole-problem-with-variable-reward-7ad9a0dcf6d0\n",
    "# https://deeplizard.com/learn/video/ewRw996uevN\n",
    "\n",
    "# Deviations from the algorithms described:\n",
    "# 1) I didnt sample from the replay memory but instead just used the whole memory and flushed it afterwards\n",
    "# 2) I scored the current state using the formula given in the towards data science article\n",
    "#    gave reward based on the difference between the score before and after the action was taken\n",
    "#    reason is that we want to reward the action improving the state that we are in\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import random\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "\n",
    "train_episodes = 100\n",
    "test_episodes = 3\n",
    "\n",
    "#definition of epsilon greedy: random action with probability epsilon\n",
    "#Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "#Epsilon of 1 means we are exploring all the time\n",
    "\n",
    "epsilon = 1 \n",
    "max_epsilon = 1 \n",
    "min_epsilon = 0.01 \n",
    "decay = 0.01\n",
    "\n",
    "#discount rate for future rewards\n",
    "gamma = 0.9\n",
    "\n",
    "#update the target network every 10 episodes\n",
    "target_update_episodes = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea41632",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#define model\n",
    "#model has input of shape of observations\n",
    "#model outputs the q values (expected rewards for action taken in that state)\n",
    "#for cart-pole, \n",
    "#input vector is a vector of 4 [cart pos, cart vel, pole angle, pole tip vel] \n",
    "#output is a vector of 2 [move left, move right]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(24, input_shape=(4,), activation='relu'))\n",
    "model.add(keras.layers.Dense(12, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['mse'])\n",
    "\n",
    "#define target model\n",
    "target_model = keras.Sequential()\n",
    "target_model.add(keras.layers.Dense(24, input_shape=(4,), activation='relu'))\n",
    "target_model.add(keras.layers.Dense(12, activation='relu'))\n",
    "target_model.add(keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "target_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['mse'])\n",
    "\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "def get_next_action(state):\n",
    "    if np.random.random() >= epsilon:\n",
    "        return np.argmax(model.predict(state.reshape(1,-1)))\n",
    "    else:\n",
    "        return np.random.randint(2)\n",
    "    \n",
    "def perform_evaluation():\n",
    "    scores = []\n",
    "\n",
    "    for i in range(test_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_score = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            env.render()\n",
    "            action = np.argmax(model.predict(state.reshape(1,-1)))\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_score += 1\n",
    "\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca54fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 Epsilon: 0.9147918734185159 Eval: 94.0\n",
      "Episode: 20 Epsilon: 0.8286895426039287 Eval: 25.666666666666668\n",
      "Episode: 30 Epsilon: 0.7507809319027796 Eval: 25.666666666666668\n",
      "Episode: 40 Epsilon: 0.680286305753183 Eval: 120.0\n",
      "Episode: 50 Epsilon: 0.616500130242572 Eval: 72.33333333333333\n",
      "Episode: 60 Epsilon: 0.558784011887162 Eval: 51.0\n",
      "Episode: 70 Epsilon: 0.5065603083753949 Eval: 157.66666666666666\n",
      "Episode: 80 Epsilon: 0.4593063473295323 Eval: 153.0\n",
      "Episode: 90 Epsilon: 0.41654919522482203 Eval: 91.0\n",
      "Episode: 100 Epsilon: 0.37786092411182526 Eval: 315.3333333333333\n",
      "Saving Model...\n",
      "INFO:tensorflow:Assets written to: cartpole_DQN_model/model\\assets\n"
     ]
    }
   ],
   "source": [
    "replay_memory = []\n",
    "best_result = 0\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    #we are giving a higher score for position and angle being closer to zero\n",
    "    #normalising by twice the max**2 (2.4 for pos and 0.2095 for angle)\n",
    "    score_old = 1 - (state[0]**2) / 11.52 - (state[2]**2) / 0.0877805\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "        action = get_next_action(state)\n",
    "        \n",
    "        #env.step returns obs,reward,done,info\n",
    "        next_state,_,done,_ = env.step(action)\n",
    "        \n",
    "        score = 1 - (state[0]**2) / 11.52 - (state[2]**2) / 0.0877805\n",
    "        reward = score - score_old\n",
    "        \n",
    "        replay_memory.append(np.concatenate((state, [action], [reward], next_state)))\n",
    "        \n",
    "        score_old = score\n",
    "        state = next_state\n",
    "        \n",
    "        if len(replay_memory) == batch_size:\n",
    "            \n",
    "            replay_memory = np.array(replay_memory)\n",
    "            \n",
    "            current_states = replay_memory[:,:4]\n",
    "            actions = replay_memory[:,4]\n",
    "            rewards = replay_memory[:,5]\n",
    "            next_states = replay_memory[:,6:]\n",
    "            \n",
    "            replay_memory = []\n",
    "            \n",
    "            current_q_values = model.predict(current_states)\n",
    "            next_q_values = target_model.predict(next_states)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                current_q_values[i,int(actions[i])] = rewards[i] + (gamma*np.max(next_q_values[i,:])) \n",
    "            \n",
    "            model.fit(current_states, current_q_values, verbose = 0)\n",
    "            #target_model.set_weights(model.get_weights())\n",
    "            \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "    \n",
    "    if (episode+1) % target_update_episodes == 0:\n",
    "        \n",
    "        target_model.set_weights(model.get_weights())\n",
    "        result = perform_evaluation()\n",
    "        \n",
    "        print(\"Episode: \" + str(episode+1)+ \" Epsilon: \" + str(epsilon) + \" Eval: \" + str(result))\n",
    "        \n",
    "        if result > 200 and result > best_result:\n",
    "            best_result = result\n",
    "            print(\"Saving Model...\")\n",
    "            model.save('cartpole_DQN_model/model')\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4efc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362.6\n"
     ]
    }
   ],
   "source": [
    "#Checking what the model is doing\n",
    "\n",
    "model = tf.keras.models.load_model('cartpole_DQN_model/model')\n",
    "test_episodes = 10\n",
    "result = perform_evaluation()\n",
    "print(result)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
